---
title: "Seminar 1: Exercises"
subtitle: "LSE MY459: Quantitative Text Analysis"
date-modified: "27 January 2025" 
toc: true
format: html
execute:
  eval: false
---

## Part 1: fun with digital text

1. There is a file in the week 2 folder on GitHub (see <https://github.com/lse-my459/lectures/tree/master/week02>) called `news_article.txt`. Read the file into R using `read_file()` in `tidyverse`/`reader` and print the string you just loaded using the `cat()` function, as well as the `print()` function.

```{r}
## Your code goes here



```

2. Figure out the encoding of the file.

```{r}
## Your code goes here
```

3. Re-read the file specifying the correct encoding and print the text using both `cat()` and `print()`. What language does it appear to be?

```{r}
## Your code goes here
```

4. Save a new file called `news_article_UTF8.txt` with UTF-8 encoding so that future you will be able to access the text with no problems. Try to open each file using a plain text editor on your computer and notice the difference!

```{r}
## Your code goes here
```


## Part 2: load tweet data

1. Load the Trump tweets and convert the resulting object as a tibble using `tibble()` function.

```{R}
## Read in librarites

library(tibble)
library(streamR)

install.packages("rtweet")
library(rtweet)
library(tidyverse)

```

```{r}
## Your code goes here

#Set the local path
tweets <- paste0("/Users/jarem/OneDrive/Dokumenty/LSE (personal cloud)/Coursework/MY459 Quant text analysis/Github materials/week02/personal notes/trump-tweets.json") %>%
  parseTweets() %>%
  tibble()

```
```{r}

head(tweets)
colnames(tweets)

```


```



2. Find the column corresponding to the date and time of each tweet and format it as a date-time object. See <https://lubridate.tidyverse.org/articles/lubridate.html#parsing-dates-and-times>. How many tweets are posted at exactly the same time as another tweet? Hint: to see if there are tweets posted at the same time, use the [`count()`](https://dplyr.tidyverse.org/reference/count.html) function on the date-time column you just created.

```{r}
## Your code goes here

tweets <- tweets %>% 
  mutate(created_at = str_replace(created_at, "^[A-z]+ ([A-z]+) ([0-9]+) ([^ ]+) .+?([0-9]+)$", "\\1 \\2 \\4 \\3")) %>% 
  mutate(created_at = mdy_hms(created_at))

tweets %>% 
  count(created_at) %>%
  arrange(desc(n)) %>%
  filter(n > 1) %>%
  select(n) %>%
  sum() %>%
  paste("There are", ., "tweets posted at the same time as another tweet.")


```

3. Arrange the dataframe in ascending order by date and then ascending order by tweet text using `arrange()`.

```{r}
## Your code goes here

tweets <- tweets %>% 
  arrange(created_at, text)

colnames(tweets)
```

4. Trump first became US president at 12:00 Eastern US time on 20th January 2017. Filter out any tweet posted before Trump became president. Hint: all listed times are UK times. 

```{r}
## Your code goes here

tweets <- tweets %>%
    filter(created_at >= ymd_hm("2017-01-20 17:00"))
    
```

## Part 3: basic text manipulations

1. Print the text of the first tweet he posted as US president. 

```{r}
## Your code goes here

print(tweets$text[1])
cat(tweets$text[1])

```

2. Find the tweet Trump posted at 12:55 pm Eastern time on that day and print it here. You should use the `cat()` function to print, and not the `print()` function.

```{r}
## Your code goes here

tweets$created_at <- ymd_hms(tweets$created_at)

tw1 <- tweets %>% 
  filter(created_at >= ymd_hm("2017-01-20 17:55")) %>%
  filter(row_number()==1) %>%
  select(text)
tw1 <- tw1$text  
cat(tw1)

```

3. Manually tokenise this tweet using any white space by splitting the string using the relevant function in `stringr`. You should end up with a character vector containing each of the tokens. You might find the [`stringr` cheat sheet](https://github.com/rstudio/cheatsheets/blob/main/strings.pdf) to be useful! How many tokens are in this tweet?

```{r}
## Your code goes here

token <- tw1 %>%
  str_split("\\s+") %>% 
  .[[1]]
print(token)
length(token)
  

```

4. Emojis do not always show up nicely. Replace any emojis in these tokens with a "place holder" like `<smiley emoji>`.

```{r}
## Your code goes here

token <- token %>%
  str_replace_all("[#]MAGA.+", "#MAGA<us flag emoji>")

```

5. Clean up the formatting of these tokens: capitalisation, punctuation, junk html code, etc. Be sure to retain any punctuation you might think is important, like hashtags or punctuation used to make emoji placeholders. Print the resulting vector of tokens. 

```{r}
## Your code goes here

token <- token %>%
  str_to_lower() %>%
  str_replace_all("^&#?[a-z]+;$", " ") %>% #remove all HTML symbols
  str_replace_all("[^A-z#<> ]", "") %>%
  str_squish() %>% # remove excess white space
  .[.!=""] # remove empyu strings
  
print(token)


```

## Part 4: removing stop words

1. Load a list of English stop words from the `quanteda` package, and assign it the name `engsw`.

```{r}

library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)

```


```{r}
## Your code goes here

engsw <- stopwords("english")
print(engsw)

```

2. Remove every token from the list of tokens that is a stop word (as defined by the list of stop words you loaded above).

```{r}
## Your code goes here

token <- token[!token %in% engsw]
print(token)


```

## Part 5: creating equivalence classes

1. Use the Snowball stemmer to stem the words in the list of tokens you created above. You can do this using the [`tokens_wordstem()`](https://quanteda.io/reference/tokens_wordstem.html) function in `quanteda`. Note: to use this function, you must first convert your vector of tokens into a `quanteda` `tokens` object using `as.tokens()`. See <https://quanteda.io/reference/as.tokens.html>. Note: you will need to make your token vector a list first using `list()`. Be sure to convert your object back into a character vector once you are done stemming.^[To use the stemmer in `quanteda`, your object must be a `tokens` object. So, for this question, we are converting our "regular" character vector into a `tokens` object to use the stemmer, then converting it back to a character vector.]

```{r}
## Your code goes here

tokens <- token %>%
  list() %>%
  as.tokens() %>%
  tokens_wordstem() %>%
  .[["text1"]]
print(tokens)


```

2. How big is the vocabulary in your document after all these preprocessing steps? 

```{r}
## Your code goes here

length(tokens)

```

## Part 6: using `quanteda` to make a DFM of all tweets

1. In your dataframe from part 2, create a unique document ID based on the tweet date and keep only the columns with this unique ID and the text of the tweet.

```{r}
## Your code goes here

tweets <- tweets %>%
  group_by(created_at) %>%
  mutate(doc_id = paste0(created_at," [", row_number(),"]")) %>%
  ungroup() %>%
  select(doc_id, text)

```


2. Find the most common bigrams, trigrams and 4-grams in the corpus of tweets. From the lists, choose two n-grams that you would like to keep together in your corpus. Then, manipulate the text so that they stay together when you tokenise. 

```{r}
## Your code goes here

tweets %>% 
  corpus() %>% 
  textstat_collocations(size=2)
  
tweets %>% 
  corpus() %>% 
  textstat_collocations(size=3)
  
tweets %>% 
  corpus() %>% 
  textstat_collocations(size=4)

tweets$text <- str_replace_all(tweets$text, "[Nn]orth\\s[kK]orea", "North_Korea") 
tweets$text <- str_replace_all(tweets$text, "[Uu]nited\\s[Ss]tates\\s[Ss]upreme\\s[Cc]ourt", "United_States_Supreme_Court")


```

2. Create a DFM using pipes. You should explicitly write out each of the arguments available for `tokens()` and `dfm()`, choosing the options that make sense for this context. Provide comments indicating your preprocessing choices, including if you keep the default. Keep in mind that every QTA project has its own requirements. One thing we will expect from you in this course is that you are explicit about which preprocessing options you choose, and that they make sense for your context.

```{r}
## Your code goes here

## Ryan's answer
tweet.dfm <- tweets %>% 
  corpus() %>% 
  tokens(what = "word", # keep default
         remove_punct = TRUE, # we're using bag of words; don't need punctuation
         remove_symbols = TRUE, # we're going to analyse words, not symbol use
         remove_numbers = TRUE, # we're going to analyse words, not numbers
         remove_url = TRUE, # many of these tweets have urls in them; not useful
         remove_separators = TRUE, # keep default (read docs)
         split_hyphens = FALSE, # we want to keep hyphenated words as is to keep their compound meaning
         split_tags = FALSE, # keep hashtags and usernames intact
         include_docvars = TRUE, # keep default
         padding = FALSE, # keep default (read docs for use case)
         concatenator = "_", # keep default (used to paste together n-grams)
         verbose = quanteda_options("verbose")) %>% # keep default
  tokens_remove(engsw) %>%
  tokens_wordstem() %>% 
  dfm(tolower = TRUE, # yes, we want to lower case every token!
      remove_padding = FALSE, # keep default, but doesn't matter since we didn't add padding when tokenising
      verbose = quanteda_options("verbose")) # keep default

```

3. How many documents in this DFM and how big is the vocabulary?

```{r}
## Your code goes here

nrow(tweet.dfm)
ncol(tweet.dfm)

```

4. Remove any feature that is used in less than 3 documents or is used less than 3 times total. How many features were removed from the vocabulary?

```{r}
## Your code goes here

tweet.dfm <- tweet.dfm %>%
  dfm_trim(min_termfreq = 3, min_docfreq = 3)

```

5. Using this smaller DFM, now make a second DFM that uses tf-idf weighting.

```{r}
## Your code goes here

tweet.dfm.w <- tweet.dfm %>%
  dfm_tfidf()


```

## Part 7: descriptive statistics

1. What are the most used features in the coupus (using the weighted DFM)? Do you see any potential problems with your preprocessing?

```{r}
## Your code goes here

tweet.dfm.w %>% 
  topfeatures()

```

2. Plot two word clouds of this corpus: one using the unweighted DFM, the other using the weighted DFM. See any major differences?

```{r}
## Your code goes here

tweet.dfm %>% 
  textplot_wordcloud()
  
  
tweet.dfm.w %>% 
  textplot_wordcloud()

```

3. Demonstrate Zipf's law in this (preprocessed) set of documents by plotting Word Frequency (y-axis) against Word Frequency Rank (x-axis). Use the unweighted DFM.

```{r}
## Your code goes here

tweet.dfm %>%
  colSums() %>% 
  sort(., decreasing=TRUE) %>%
  tibble(rank=1:length(.), freq = unname(.), word=names(.)) %>%
  select(-.) %>%
  ggplot(aes(x=rank,y=freq,label=word)) + 
  geom_point() + 
  geom_text(aes(label=ifelse(rank<=6,as.character(word),'')), hjust=0, vjust=0, nudge_x = 4, nudge_y = 3) + 
  labs(title="Zipf's law for Trump's January 2017 tweets") + 
  ylab("Word Frequency") + xlab("Word Frequency Rank") + 
  theme_bw()


```

4. Measure the readability of each tweet using the Flesch-Kincaid index. Print a tweet with a readability score of 1, and another with a readability score of 12:

```{r}
## Your code goes here

fk <- tweets %>%
  corpus() %>%
  textstat_readability(measure = "Flesch.Kincaid") 
  
fklh <- fk %>%
  filter((0.9 < Flesch.Kincaid & Flesch.Kincaid < 1.1) | (11.9 < Flesch.Kincaid & Flesch.Kincaid < 12.1)) %>%
  mutate(Flesch.Kincaid = round(Flesch.Kincaid)) %>%
  group_by(Flesch.Kincaid) %>% 
  filter(row_number() == 1) %>% 
  arrange(Flesch.Kincaid)
print(paste0("Most readable: ", tweets$text[tweets$doc_id == fklh$document[1]]))
print(paste0("Least readable: ", tweets$text[tweets$doc_id == fklh$document[2]]))


```